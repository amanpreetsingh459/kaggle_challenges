{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AmY4yPMzHJm"
      },
      "outputs": [],
      "source": [
        "# Installation (please use latest versions if stucked anywhere)\n",
        "!pip install -q accelerate==0.34.2\n",
        "!pip install -q peft==0.14.0\n",
        "!pip install -q bitsandbytes==0.45.0\n",
        "!pip install -q transformers==4.47.1\n",
        "!pip install -q trl==0.13.0\n",
        "#!pip install -q fastrlock==0.8.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os, torch, logging\n",
        "from datasets import load_dataset, concatenate_datasets, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    EarlyStoppingCallback)\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "lcIYFZB5zTYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set enironment variables (secrets if working in colab)\n",
        "os.environ['HF_TOKEN'] = '<your-huggingface-token>'\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "tzHb7V65zTTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# below is the huggingface directory location to load the model from\n",
        "# you can download the model in local and give the location to that as well\n",
        "finetuned_model_name = \"amanpreetsingh459/gemma-2-2b-punjabi-finetuned-4\""
      ],
      "metadata": {
        "id": "y6X9-0N-zTNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_name)\n",
        "EOS_TOKEN = gemma_tokenizer.eos_token"
      ],
      "metadata": {
        "id": "DFVUKjz6zTIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_finetuned = AutoModelForCausalLM.from_pretrained(\n",
        "    finetuned_model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "xkWN_xGJzTEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "pUm2H2CezS_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = gemma_tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"ਮੇਨੂ ਏਕ ਕਵਿਤਾ ਲਿੱਖ ਕੇ ਦੇਯੋ ਜੀ\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "VKhgl-dVzgbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model_finetuned.generate(**inputs, max_new_tokens = 250)"
      ],
      "metadata": {
        "id": "Nj_fZOATzgXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_outputs = gemma_tokenizer.batch_decode(outputs)\n",
        "print(decoded_outputs[0])"
      ],
      "metadata": {
        "id": "e3_7sMY5zgST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wB1tYCT3znei"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}